{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "increased-grocery",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#Imports and packages needed\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import script.functions as func\n",
    "import pickle\n",
    "import autoreload \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from hpsklearn import HyperoptEstimator, multinomial_nb, tfidf\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from hyperopt import hp, Trials, tpe, fmin, space_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "solid-briefing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Data \n",
    "train = pd.read_csv('../data/clean_train.csv', index_col=0)\n",
    "test = pd.read_csv('../data/clean_test.csv', index_col=0)\n",
    "\n",
    "#Separate dependent and independent variables and split data\n",
    "X = train['Phrase']\n",
    "y = train['Sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "still-fighter",
   "metadata": {},
   "source": [
    "## Naive Bayes Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "antique-virus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.47s/trial, best loss: 0.4835557673975215]\n",
      "100%|██████████| 2/2 [00:02<00:00,  2.63s/trial, best loss: 0.45527486495074676]\n",
      "100%|██████████| 3/3 [00:02<00:00,  2.58s/trial, best loss: 0.45527486495074676]\n",
      "100%|██████████| 4/4 [00:03<00:00,  3.15s/trial, best loss: 0.45527486495074676]\n",
      "100%|██████████| 5/5 [00:00<00:00,  1.06trial/s, best loss: 0.45527486495074676]\n",
      "100%|██████████| 6/6 [00:02<00:00,  2.57s/trial, best loss: 0.45527486495074676]\n",
      "100%|██████████| 7/7 [00:01<00:00,  1.95s/trial, best loss: 0.45527486495074676]\n",
      "100%|██████████| 8/8 [00:01<00:00,  1.93s/trial, best loss: 0.45527486495074676]\n",
      "100%|██████████| 9/9 [00:02<00:00,  2.73s/trial, best loss: 0.45527486495074676]\n",
      "100%|██████████| 10/10 [00:01<00:00,  2.00s/trial, best loss: 0.45527486495074676]\n"
     ]
    }
   ],
   "source": [
    "#Optimizing using hyper-opt sklearn\n",
    "estimator = HyperoptEstimator(classifier= multinomial_nb('nb'),\n",
    "                              preprocessing = [tfidf('tf')], \n",
    "                             algo=tpe.suggest)\n",
    "estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "champion-anatomy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.7751561084894418\n",
      "Test Score: 0.5530505243088656\n"
     ]
    }
   ],
   "source": [
    "print(f'Train Score: {estimator.score(X_train, y_train)}')\n",
    "print(f'Test Score: {estimator.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "competitive-turkish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:42<00:00,  4.26s/trial, best loss: 0.44212458726097004]\n"
     ]
    }
   ],
   "source": [
    "#Optimizing using hyper-opt\n",
    "pipe = Pipeline([('tf_vec', TfidfVectorizer()),\n",
    "                ('nb', MultinomialNB())])\n",
    "\n",
    "space = {}\n",
    "space['tf_vec__ngram_range'] = hp.choice('tf_vec__ngram_range', [(1,1), (1,2), (1,3)])\n",
    "space['tf_vec__stop_words'] = hp.choice('tf_vec__stop_words', [None, 'english'])\n",
    "space['tf_vec__min_df'] = hp.randint('tf_vec__min_df', 3)\n",
    "space['tf_vec__max_df'] = hp.uniform('tf_vec__max_df', 0.7, 1.0)\n",
    "space['nb__alpha'] = hp.loguniform('nb__alpha', 0,1)\n",
    "\n",
    "def objective(params):\n",
    "    pipe.set_params(**params)\n",
    "    score = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "    return 1 - score.mean()\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(objective,\n",
    "            space,\n",
    "            algo = tpe.suggest,\n",
    "           max_evals=10,\n",
    "           trials=trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "subsequent-nancy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.6374310818755263\n",
      "Test Score: 0.5600095328884652\n"
     ]
    }
   ],
   "source": [
    "best_params = space_eval(space, best)\n",
    "pipe.set_params(**best_params)\n",
    "pipe.fit(X_train, y_train)\n",
    "print(f'Train Score: {pipe.score(X_train, y_train)}')\n",
    "print(f'Test Score: {pipe.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bright-department",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pickle the models\n",
    "pickle.dump(estimator, open( \"../models/nb_1.pkl\", \"wb\" ))\n",
    "pickle.dump(pipe, open( \"../models/nb_2.pkl\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-standard",
   "metadata": {},
   "source": [
    "## LSTM Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "minor-special",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Bidirectional, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "armed-reminder",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing \n",
    "X_train, X_train_df = func.encode_and_pad(pd.DataFrame(X_train), 30000, 200)\n",
    "X_test, X_test_df = func.encode_and_pad(pd.DataFrame(X_test), 30000, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "compound-destination",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "economic-miller",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_16 (Embedding)     (None, None, 128)         3840000   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "bidirectional_33 (Bidirectio (None, None, 128)         98816     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "bidirectional_34 (Bidirectio (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 4,038,277\n",
      "Trainable params: 4,038,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(30000, 128, input_shape=(None,)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunrise-criterion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 62937 samples, validate on 20980 samples\n",
      "Epoch 1/30\n",
      " 3000/62937 [>.............................] - ETA: 9:08 - loss: 1.5082 - acc: 0.4690"
     ]
    }
   ],
   "source": [
    "model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "result = model.fit(X_train, y_train, batch_size=500, epochs=30, validation_data=(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
