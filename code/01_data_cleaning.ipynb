{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daily-wallace",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "**Note:** codes use in this step can be found in the script notebook\n",
    "\n",
    "---\n",
    "\n",
    "### Dataset: \n",
    "The dataset used for this project is from the Kaggle competition [Sentiment Analysis of Move Reviews](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews). It is a corpus of movie reviews broken into phrases of various lengths accompanied by a sentiment rating ranging from 0 to 4, where zero (0) is negative and four (4)  is positive. There is a total of 83917 training data with four data attributes (phraseid, sentenceid, phrase, and sentiment). In contrast, the testing dataset is composed of 36856 samples. \n",
    "\n",
    "### Cleaning: \n",
    "We initially examined for duplicated and null values in the dataset using the `initial_check` function. Feature types are also considered in case it needed corrections.  We then used a second function called `text_preprocessing` to perform regex cleaning for the Phrase column, removing websites, punctuations, and stopwords. It also normalizes the phrases to only contain lowercase letters (regex code is taken from this [link](https://www.kaggle.com/ankitkumarsaini/distilbert). Since regex was conducted, it resulted in some blank and duplicated rows. The function `text_processing` will also correct this. \n",
    "\n",
    "```\n",
    "def text_cleaning(text):\n",
    "    forbidden_words = set(stopwords.words('english'))\n",
    "    if text:\n",
    "        text = ' '.join(text.split('.'))\n",
    "        text = re.sub('\\/',' ',text)\n",
    "        text = re.sub(r'\\\\',' ',text)\n",
    "        text = re.sub(r'((http)\\S+)','',text)\n",
    "        text = re.sub(r'\\s+', ' ', re.sub('[^A-Za-z]', ' ', text.strip().lower())).strip()\n",
    "        text = re.sub(r'\\W+', ' ', text.strip().lower()).strip()\n",
    "        text = [word for word in text.split() if word not in forbidden_words]\n",
    "    return \" \".join(text)\n",
    "```\n",
    "\n",
    "```\n",
    "def text_processing(dataframe):\n",
    "    df = dataframe.copy()\n",
    "    #Cleans the dataframe using the text_cleaning function (regex)\n",
    "    df['Phrase'] = df['Phrase'].map(text_cleaning)\n",
    "    #Removes rows that have blank in Phrase\n",
    "    df = df.loc[df['Phrase'].map(lambda x: len(x.split())) > 0]\n",
    "    #Remove duplicates\n",
    "    df = df.drop_duplicates(subset=['Phrase'])\n",
    "    return df\n",
    "```\n",
    "The cleaned dataset is saved as a CSV file in the data directory under the name clean_train and clean_test. The final sample count for the train data is 156060 and 66292 test samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "grand-sweden",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from script import functions as func\n",
    "import autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "nervous-second",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicated values: 0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 156060 entries, 0 to 156059\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   PhraseId    156060 non-null  int64 \n",
      " 1   SentenceId  156060 non-null  int64 \n",
      " 2   Phrase      156060 non-null  object\n",
      " 3   Sentiment   156060 non-null  int64 \n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 4.8+ MB\n",
      "None\n",
      "Duplicated values: 0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 66292 entries, 0 to 66291\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   PhraseId    66292 non-null  int64 \n",
      " 1   SentenceId  66292 non-null  int64 \n",
      " 2   Phrase      66292 non-null  object\n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 1.5+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Import Data \n",
    "train = func.initial_check('../data/train.tsv', delimiter='\\t')\n",
    "test = func.initial_check('../data/test.tsv', delimiter='\\t')\n",
    "\n",
    "#Processs data (Please see description above)\n",
    "train = func.text_processing(train)\n",
    "test = func.text_processing(test)\n",
    "\n",
    "#Save clean data \n",
    "train.to_csv('../data/clean_train.csv')\n",
    "test.to_csv('../data/clean_test.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
